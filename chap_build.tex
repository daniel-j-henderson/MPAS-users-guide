\chapter{Building MPAS}
\label{chap:mpas_build_instructions}

\section{Prequisites}
\label{build_prerequisites}

To build MPAS, compatible C and Fortran compilers are required. Additionally,
the MPAS software relies on the PIO parallel I/O library to read and write model
fields, and the PIO library requires the standard NetCDF library as well as the
Parallel-NetCDF library from Argonne National Labs. All libraries must be
compiled with the same compilers that will be used to build MPAS. Section
\ref{sec:build_io} summarizes the basic procedure of installing the required I/O
libraries for MPAS.

In order for the MPAS makefiles to find the PIO, Parallel-NetCDF, and NetCDF
include files and libraries, the environment variables {\tt PIO}, {\tt PNETCDF},
and {\tt NETCDF} should be set to the root installation directories of the PIO,
Parallel-NetCDF, and NetCDF installations, respectively. 

An MPI installation such as MPICH or OpenMPI is also required, and there is no
option to build a serial version of the MPAS executables. MPAS-Atmosphere v5.0
introduces the capability to use hybrid parallelism using MPI and OpenMP; however,
the use of OpenMP {\em should be considered experimental} and generally does not
offer any performance advantage. The primary reason for releasing a shared-memory
capability is to make this code available to collaborators for future development.


\section{Compiling I/O Libraries}
\label{sec:build_io}

{\bf IMPORTANT NOTE:} {\em The instructions provided in this section for
installing libraries have been successfully used by MPAS developers, but due to
differences in library versions, compilers, and system configurations, it is
recommended that users consult documentation provided by individual library
vendors should problems arise during installation. The MPAS developers take no
responsibility for third-party libraries.} \vspace{12pt}

Although most recent versions of the NetCDF and Parallel-netCDF libraries should work,
the most tested versions of these libraries are NetCDF 4.1.3 and Parallel-NetCDF 1.3.1.
Users are strongly encouraged to use PIO versions 1.7.1 or 1.9.23, as other versions
have not been tested or are known to not work with MPAS. The NetCDF and Parallel-NetCDF
libraries must be installed before building the PIO library.

\subsection{NetCDF}
\label{serial_netcdf}

Version 4.1.3 of the NetCDF library may be downloaded from
\url{http://www.unidata.ucar.edu/downloads/netcdf/netcdf-4\_1\_3/index.jsp}.
Assuming the gfortran and gcc compilers will be used, the following shell
commands (in csh/tcsh) are generally sufficient to install NetCDF. Of course, other compilers
may be used by making suitable substitutions in the commands, below.

\vspace{12pt}
{\tt > setenv FC gfortran}

{\tt > setenv F77 gfortran} 

{\tt > setenv F90 gfortran}

{\tt > setenv CC gcc} 

{\tt > ./configure --prefix=XXXXX --disable-dap --disable-netcdf-4 --disable-cxx \hfill\break --disable-shared --enable-fortran} 

{\tt > make all check}

{\tt > make install}
\vspace{12pt}

Here, {\tt XXXXX} should be replaced with the directory that will serve as the
root installation directory for NetCDF.  {\em Before proceeding to compile PIO
the {\tt NETCDF\_PATH} environment variable should be set to the NetCDF root
installation directory.}

Certain compilers require addition flags in the CPPFLAGS environment variable.
Please refer to the NetCDF installation instructions for these flags.

\subsection{Parallel-NetCDF}
\label{parallel_netcdf}

Version 1.3.1 of the Parallel-NetCDF library may be downloaded from
\url{https://trac.mcs.anl.gov/projects/parallel-netcdf/wiki/Download}. Again, assuming
the gfortran and gcc compilers will be used, the following shell commands are
generally sufficient to install Parallel-NetCDF; environment variables set during the
installation of the NetCDF library are assumed to be already set.

\vspace{12pt}
{\tt > setenv MPIF90 mpif90}

{\tt > setenv MPIF77 mpif90} 

{\tt > setenv MPICC mpicc}  

{\tt > ./configure --prefix=XXXXX} 

{\tt > make}

{\tt > make install}
\vspace{12pt}

Here, {\tt XXXXX} should be replaced with the directory that will serve as the
root installation directory for Parallel-NetCDF.  {\em Before proceeding to
compile PIO the {\tt PNETCDF\_PATH} environment variable should be set to the
Parallel-NetCDF root installation directory.}


\subsection{PIO}
\label{pio_build}

Due to the rapid development pace of the PIO library, it is recommended to
obtain and install PIO following the instructions at
\url{http://www.cesm.ucar.edu/models/pio/} for building PIO.

After PIO is built and installed the {\tt PIO} enviroment variable should be set to 
the directory where PIO was installed. Recent versions of PIO support a {\tt --prefix}
option, which specifies the installation directory, while older versions do not,
in which case the {\tt PIO} environment variable should be set to the directory where
PIO was compiled.

\section{Compiling MPAS}
\label{compiling_MPAS}

{\bf IMPORTANT NOTE:} {\em Before compiling MPAS, the {\tt NETCDF}, {\tt
PNETCDF}, and {\tt PIO} environment variables must be set to the library
installation directories as described in the previous section.} \vspace{12pt}

The MPAS code uses only the `make' utility for compilation. Rather than
employing a separate configuration step before building the code, all
information about compilers, compiler flags, etc., is contained in the top-level
{\tt Makefile}; each supported combination of compilers (i.e., a configuration)
is included in the {\tt Makefile} as a separate make target, and the user
selects among these configurations by running {\tt make} with the name of a
build target specified on the command-line, e.g.,

\vspace{12pt}
{\tt > make gfortran}
\vspace{12pt}

\noindent to build the code using the GNU Fortran and C compilers. Some of the
available targets are listed in the table below, and additional targets can be
added by simply editing the {\tt Makefile} in the top-level directory.

\vspace{12pt}
\begin{longtable}{| l | l | l | l |}
\hline
Target & Fortran compiler & C compiler & MPI wrappers \\ \hline \hline
{\tt xlf} & xlf90 & xlc & mpxlf90 / mpcc \\ \hline
{\tt pgi} & pgf90 & pgcc & mpif90 / mpicc \\ \hline
{\tt ifort} & ifort & gcc & mpif90 / mpicc \\ \hline
{\tt gfortran} & gfortran & gcc & mpif90 / mpicc \\ \hline
{\tt bluegene} & bgxlf95\_r & bgxlc\_r & mpxlf95\_r / mpxlc\_r \\ \hline
\end{longtable}
\vspace{12pt}

The MPAS framework supports multiple {\em cores} --- currently a shallow water
model, an ocean model, a land-ice model, a non-hydrostatic atmosphere model, and
a non-hydrostatic atmosphere initialization core --- so the build process must be told which core
to build. This is done by either setting the environment variable {\tt CORE} to
the name of the model core to build, or by specifying the core to be built
explicitly on the command-line when running {\tt make}. For the atmosphere
core, for example, one may run either

\vspace{12pt}
{\tt > setenv CORE atmosphere}

{\tt > make gfortran}
\vspace{12pt}

\noindent or

\vspace{12pt}
{\tt > make gfortran CORE=atmosphere}
\vspace{12pt}

If the {\tt CORE} environment variable is set and a core is specified on the
command-line, the command-line value takes precedence; if no core is specified,
either on the command line or via the {\tt CORE} environment variable, the build
process will stop with an error message stating such.  Assuming compilation is
successful, the model executable, named {\tt \$\{CORE\}\_model} (e.g., {\tt
atmosphere\_model}), should be created in the top-level MPAS directory.

In order to get a list of available cores, one can simply run the top-level {\tt
Makefile} without setting the {\tt CORE} environment variable or passing the
core via the command-line. An example of the output from this can be seen
below.

{\small
\begin{verbatim}
> make
( make error )
make[1]: Entering directory `/disk1/MPAS-Release'

Usage: make target CORE=[core] [options]

Example targets:
    ifort
    gfortran
    xlf
    pgi

Availabe Cores:
    atmosphere
    init_atmosphere
    landice
    ocean
    sw
    test

Available Options:
    DEBUG=true    - builds debug version. Default is optimized version.
    USE_PAPI=true - builds version using PAPI for timers. Default is off.
    TAU=true      - builds version using TAU hooks for profiling. Default is off.
    AUTOCLEAN=true    - forces a clean of infrastructure prior to build new core.
    GEN_F90=true  - Generates intermediate .f90 files through CPP, and builds with them.
    TIMER_LIB=opt - Selects the timer library interface to be used for profiling the model.
                    Options are:
                    TIMER_LIB=native - Uses native built-in timers in MPAS
                    TIMER_LIB=gptl - Uses gptl for the timer interface instead of
                                     the native interface
                    TIMER_LIB=tau - Uses TAU for the timer interface instead of
                                    the native interface
    OPENMP=true   - builds and links with OpenMP flags. Default is to not use OpenMP.
    USE_PIO2=true - links with the PIO 2 library. Default is to use the PIO 1.x library.
    PRECISION=single - builds with default single-precision real kind. Default is to use
                       double-precision.

Ensure that NETCDF, PNETCDF, PIO, and PAPI (if USE_PAPI=true) are environment variables
that point to the absolute paths for the libraries.

************ ERROR ************
No CORE specified. Quitting.
************ ERROR ************
\end{verbatim}
}

\section{Selecting a single-precision build}

Beginning with version 2.0, MPAS-Atmosphere can be compiled and run in single-precision, offering faster 
model execution and smaller input and output files. Beginning with version 5.0, the selection of the
model precision can be made on the command-line, with no need to edit the {\tt Makefile}.
To compile a single-precision MPAS-Atmosphere executable, add {\tt PRECISION=single} to the build command, e.g.,

\vspace{12pt}
{\tt > make gfortran CORE=atmosphere PRECISION=single}
\vspace{12pt}

Note that running MPAS-Atmosphere in single-precision requires the user to begin with single-precision SCVT grid files, 
and all pre-processing steps must be run using a single-precision version of init\_atmosphere\_model with these grid files. 
In order to obtain suitable grid files, any existing double-precision SCVT grid file that was downloaded from the MPAS-Atmosphere 
meshes download page may be run through the {\tt double\_to\_float\_grid} converter program to produce a single-precision grid file. 
Using the double-precision grid files with single-precision executables will not work.

\section{Cleaning}

To remove all files  that were created when the model was built,
including the model executable itself, {\tt make} may be run for the
`clean' target:

\vspace{12pt}
{\tt > make clean}
\vspace{12pt}

As with compiling, the core to be cleaned is specified by the {\tt CORE}
environment variable, or by specifying a core explicitly on the
command-line with {\tt CORE=}.
